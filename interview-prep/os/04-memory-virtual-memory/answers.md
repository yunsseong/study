# 메모리 관리와 가상 메모리 면접 질문 + 답변

---

## 기본 개념

### Q1. 논리 주소(Logical Address)와 물리 주소(Physical Address)의 차이를 설명해주세요.

> 논리 주소는 CPU가 생성하는 가상 주소로, 프로세스가 실행되는 동안 사용하는 주소입니다. 0번지부터 시작하여 각 프로세스마다 독립적인 주소 공간을 가집니다. 물리 주소는 실제 메모리(RAM)의 주소로, 메모리 관리 장치(MMU)가 논리 주소를 물리 주소로 변환합니다. 이러한 주소 변환을 통해 프로세스 간 메모리 격리를 보장하고, 물리 메모리보다 큰 가상 메모리를 사용할 수 있으며, 메모리 보호와 공유가 가능합니다.

---

### Q2. 페이징(Paging)이 무엇이며, 왜 사용하나요?

> 페이징은 논리 메모리를 고정 크기의 페이지(Page)로, 물리 메모리를 같은 크기의 프레임(Frame)으로 나누어 관리하는 기법입니다. 일반적으로 페이지 크기는 4KB입니다. 페이지 테이블을 통해 논리 주소의 페이지 번호를 물리 주소의 프레임 번호로 매핑합니다. 외부 단편화를 해결하고, 메모리를 효율적으로 사용하며, 프로세스가 연속된 물리 메모리를 요구하지 않아도 됩니다. 다만 내부 단편화가 발생할 수 있고, 페이지 테이블이 메모리 공간을 차지합니다.

---

### Q3. 페이지 폴트(Page Fault)가 무엇이고, 어떻게 처리되나요?

> 페이지 폴트는 프로세스가 접근하려는 페이지가 물리 메모리에 없을 때 발생하는 인터럽트입니다. 발생 시 운영체제는 다음과 같이 처리합니다: 1) 유효하지 않은 접근인지 확인(세그멘테이션 폴트 발생), 2) 디스크의 스왑 영역에서 해당 페이지를 찾음, 3) 비어있는 프레임을 할당하거나 페이지 교체 알고리즘으로 기존 페이지를 내보냄, 4) 페이지를 메모리로 로드하고 페이지 테이블을 업데이트, 5) 프로세스 재개. 페이지 폴트가 자주 발생하면 성능이 저하되어 스래싱이 발생할 수 있습니다.

---

### Q4. 가상 메모리(Virtual Memory)가 무엇이며, 어떤 장점이 있나요?

> 가상 메모리는 물리 메모리와 디스크(스왑 공간)를 함께 사용하여 프로세스에게 큰 연속된 메모리 공간을 제공하는 기술입니다. 주요 장점은 첫째, 물리 메모리보다 큰 프로그램을 실행할 수 있습니다. 둘째, 각 프로세스가 독립적인 주소 공간을 가져 메모리 보호와 격리가 가능합니다. 셋째, 여러 프로세스가 동시에 실행될 수 있어 멀티프로그래밍 효율이 높아집니다. 넷째, 프로세스 간 메모리 공유가 용이합니다. Demand Paging을 통해 필요한 페이지만 메모리에 로드하여 메모리 사용을 최적화합니다.

---

### Q5. TLB(Translation Lookaside Buffer)가 무엇이고, 왜 필요한가요?

> TLB는 최근에 사용된 페이지 테이블 엔트리를 캐싱하는 하드웨어 캐시로, MMU 내부에 위치합니다. 페이지 테이블이 메모리에 있어 주소 변환마다 메모리 접근이 필요하므로, 실제 데이터 접근까지 최소 2번의 메모리 접근이 필요합니다. TLB를 사용하면 최근 사용된 페이지 번호-프레임 번호 매핑을 빠르게 찾아 메모리 접근을 1번으로 줄입니다. TLB 히트율이 높을수록(보통 99% 이상) 성능이 크게 향상됩니다. TLB 미스 시에는 페이지 테이블을 참조하고 TLB를 업데이트합니다.

---

## 비교/구분

### Q6. 내부 단편화(Internal Fragmentation)와 외부 단편화(External Fragmentation)의 차이를 설명해주세요.

> 내부 단편화는 할당된 메모리 공간이 필요한 양보다 커서 내부에 사용되지 않는 공간이 남는 현상입니다. 페이징에서 발생하며, 예를 들어 페이지 크기가 4KB인데 프로세스가 4.1KB를 요구하면 2개 페이지(8KB)를 할당하여 3.9KB가 낭비됩니다. 평균적으로 페이지당 절반(2KB)의 내부 단편화가 발생합니다. 외부 단편화는 전체 여유 메모리는 충분하지만 연속되지 않아 할당할 수 없는 현상입니다. 가변 분할 방식에서 발생하며, 압축(Compaction)으로 해결할 수 있지만 비용이 큽니다. 페이징은 외부 단편화를 해결하지만 내부 단편화가 발생합니다.

---

### Q7. LRU, FIFO, LFU 페이지 교체 알고리즘을 설명하고, 각각의 장단점을 비교해주세요.

> FIFO(First-In, First-Out)는 가장 먼저 들어온 페이지를 교체하는 가장 간단한 알고리즘입니다. 구현이 쉽지만 오래 사용된 페이지가 자주 사용될 수도 있어 성능이 낮고, Belady's Anomaly(프레임 수를 늘려도 페이지 폴트가 증가)가 발생할 수 있습니다. LRU(Least Recently Used)는 가장 오래 사용되지 않은 페이지를 교체하며, 지역성(Locality) 원리에 부합하여 성능이 좋지만 구현이 복잡하고 시간 기록을 위한 오버헤드가 있습니다. LFU(Least Frequently Used)는 사용 빈도가 가장 낮은 페이지를 교체하며, 초기에 많이 사용된 페이지가 나중에 불필요해도 남아있을 수 있습니다. 실무에서는 LRU의 근사 알고리즘을 많이 사용합니다.

---

### Q8. 페이징(Paging)과 세그멘테이션(Segmentation)의 차이를 설명해주세요.

> 페이징은 메모리를 고정 크기로 나누는 물리적 분할 방식으로, 외부 단편화가 없지만 내부 단편화가 발생하고, 페이지는 논리적 의미가 없습니다. 세그멘테이션은 프로그램을 논리적 단위(코드, 데이터, 스택)로 나누는 가변 크기 분할 방식으로, 내부 단편화가 없지만 외부 단편화가 발생합니다. 세그멘테이션은 세그먼트 단위로 보호와 공유가 용이하며 논리적으로 의미 있습니다. 현대 운영체제는 두 기법을 결합한 세그멘테이션-페이징(Segmented Paging)을 사용하여 각각의 장점을 취합니다.

---

### Q9. Demand Paging과 Prefetching의 차이는 무엇인가요?

> Demand Paging은 요구 페이징으로, 프로세스가 페이지를 실제로 접근할 때만 메모리에 로드하는 기법입니다. 초기 로딩 시간이 짧고 메모리를 효율적으로 사용하지만, 페이지 폴트가 자주 발생하면 성능이 저하됩니다. 프로세스 시작 시 필요한 최소한의 페이지만 로드하고 나머지는 필요 시 로드합니다. Prefetching은 미리 가져오기로, 앞으로 사용될 것으로 예상되는 페이지를 미리 메모리에 로드하는 기법입니다. 지역성 원리를 활용하여 페이지 폴트를 줄이지만, 예측이 틀리면 메모리 낭비와 불필요한 I/O가 발생합니다.

---

## 심화/실무

### Q10. 스래싱(Thrashing)이 무엇이며, 왜 발생하고 어떻게 해결할 수 있나요?

> 스래싱은 페이지 폴트가 과도하게 발생하여 프로세스가 실제 작업보다 페이지 교체에 더 많은 시간을 소비하는 현상입니다. 멀티프로그래밍 수준이 높아져 각 프로세스에 할당된 프레임 수가 부족하면, 페이지 폴트가 빈번하게 발생하고 디스크 I/O가 증가하여 CPU 이용률이 급격히 떨어집니다. 해결 방법은 첫째, 멀티프로그래밍 수준을 낮춰 프로세스 수를 줄입니다. 둘째, 워킹 셋(Working Set) 모델로 각 프로세스가 활발히 사용하는 페이지 집합을 메모리에 유지합니다. 셋째, 페이지 폴트 빈도(PFF)를 모니터링하여 프레임을 동적으로 조정합니다. 넷째, 물리 메모리를 증설합니다.

---

### Q11. JVM의 메모리 구조(Heap, Stack, Method Area)와 가비지 컬렉션을 설명해주세요.

> JVM 메모리는 Heap, Stack, Method Area로 나뉩니다. Heap은 동적 할당된 객체(new로 생성)가 저장되며 모든 스레드가 공유하고, 가비지 컬렉션의 대상입니다. Young Generation(Eden, Survivor)과 Old Generation으로 나뉘어 세대별 GC가 수행됩니다. Stack은 각 스레드마다 독립적으로 생성되며, 메서드 호출 시 지역 변수와 매개변수가 저장되고 메서드 종료 시 자동 해제됩니다. Method Area(Metaspace)는 클래스 메타데이터, static 변수, 상수 풀이 저장됩니다. 가비지 컬렉션은 Heap에서 더 이상 참조되지 않는 객체를 자동으로 회수하며, Mark-and-Sweep 알고리즘을 사용합니다. Minor GC는 Young Generation에서, Major GC는 Old Generation에서 발생하며 Stop-the-World가 발생할 수 있습니다.

---

### Q12. Redis의 메모리 관리 정책과 LRU 구현을 설명해주세요.

> Redis는 인메모리 데이터베이스로, maxmemory 설정으로 최대 메모리를 제한하고 메모리가 가득 차면 eviction 정책에 따라 키를 제거합니다. 주요 정책은 noeviction(쓰기 거부), allkeys-lru(모든 키 중 LRU), volatile-lru(TTL 설정된 키 중 LRU), allkeys-lfu(사용 빈도 기반), volatile-ttl(TTL이 짧은 키 제거) 등이 있습니다. Redis의 LRU는 정확한 LRU가 아닌 근사 LRU로, 전체 키를 추적하지 않고 샘플링(기본 5개)하여 그중 가장 오래된 키를 제거합니다. 이는 메모리와 성능을 절약하면서도 충분히 효과적입니다. LFU는 접근 빈도와 시간을 함께 고려하여 더 정교한 제거가 가능합니다.

---

### Q13. 다중 레벨 페이지 테이블(Multi-level Page Table)이 왜 필요한가요?

> 32비트 시스템에서 페이지 크기가 4KB이면 페이지 테이블 엔트리가 2^20개(약 100만 개) 필요하고, 각 엔트리가 4바이트면 페이지 테이블이 4MB를 차지합니다. 모든 프로세스마다 이런 테이블을 유지하면 메모리 낭비가 큽니다. 64비트 시스템에서는 더욱 심각합니다. 다중 레벨 페이지 테이블은 페이지 테이블을 계층적으로 구성하여, 사용되지 않는 영역의 페이지 테이블은 생성하지 않아 메모리를 절약합니다. 예를 들어 2단계 페이지 테이블은 외부 페이지 테이블과 내부 페이지 테이블로 나뉘며, 주소 변환 시간이 증가하지만 TLB로 완화됩니다. 현대 시스템은 4단계 이상을 사용합니다.

---

## 꼬리질문

### Q14. 페이지 크기가 크면/작으면 각각 어떤 장단점이 있나요?

> 페이지 크기가 크면 내부 단편화가 증가하지만(평균 낭비가 페이지 크기의 절반), 페이지 테이블 크기가 작아져 메모리 오버헤드가 줄고, 디스크 I/O가 줄어 페이지 폴트 처리가 빠릅니다. 또한 TLB 히트율이 높아집니다. 페이지 크기가 작으면 내부 단편화가 줄어들고 필요한 만큼만 로드하여 메모리 효율이 높아지지만, 페이지 테이블이 커지고 디스크 I/O 횟수가 증가하여 페이지 폴트 처리가 느려집니다. 일반적으로 4KB를 사용하며, 대용량 메모리를 다루는 경우 Huge Pages(2MB, 1GB)를 사용하기도 합니다.

---

### Q15. Copy-on-Write(COW)가 무엇이며, 어디에 사용되나요?

> Copy-on-Write는 fork()로 프로세스를 복제할 때 즉시 메모리를 복사하지 않고, 부모와 자식이 같은 물리 메모리를 공유하다가 한쪽이 쓰기를 시도할 때만 복사하는 기법입니다. 읽기만 하면 복사가 발생하지 않아 메모리와 시간을 절약합니다. fork() 후 자식이 exec()를 호출하여 새로운 프로그램으로 교체되는 경우가 많으므로, 불필요한 복사를 방지합니다. 페이지 테이블 엔트리에 쓰기 금지 플래그를 설정하고, 쓰기 시도 시 페이지 폴트가 발생하면 그때 페이지를 복사합니다. 파일 시스템, 가상 머신, Docker 컨테이너 등에서도 사용됩니다.
